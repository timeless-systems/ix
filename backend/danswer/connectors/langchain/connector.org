#!/usr/bin/env python3
import os
import sys
import glob
from typing import List
from multiprocessing import Pool
from tqdm import tqdm
import shutil
from typing import Any
from datetime import datetime
from datetime import datetime
from langchain.document_loaders import (
    CSVLoader,
    EverNoteLoader,
    PDFMinerLoader,
    TextLoader,
    GCSFileLoader,
    GCSDirectoryLoader,
    UnstructuredEmailLoader,
    UnstructuredEPubLoader,
    UnstructuredHTMLLoader,
    UnstructuredMarkdownLoader,
    UnstructuredODTLoader,
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader,
)
from langchain.docstore.document import Document

from danswer.configs.app_configs import DOCS_SOURCE_DIRECTORY
from danswer.configs.app_configs import DOCS_PROCESSED_DIRECTORY
from danswer.connectors.interfaces import LoadConnector

from danswer.utils.logger import setup_logger

_METADATA_FLAG = "#DANSWER_METADATA="

logger = setup_logger()

# Usage without socket connection (optional)

logger.info(f"Starting ingestion")

# Â Load environment variables
# Custom document loaders

class MyElmLoader(UnstructuredEmailLoader):
    """Wrapper to fallback to text/plain whe/Users/ts1/development/vianai/HilaEnterprise/kombucha-backend/backend/utilsn default does not work"""

    def load(self) -> List[Document]:
        """Wrapper adding fallback for elm without html"""
        try:
            try:
                doc = UnstructuredEmailLoader.load(self)
            except ValueError as e:
                if 'text/html content not found in email' in str(e):
                    # Try plain text
                    self.unstructured_kwargs["content_source"] = "text/plain"
                    doc = UnstructuredEmailLoader.load(self)
                else:
                    raise
        except Exception as e:
            # Add file_path to exception message
            raise type(e)(f"{self.file_path}: {e}") from e

        return doc


class LangchainFileConnector(LoadConnector):
    def __init__(self) -> None:
        self.logger.get_logger().info("Starting ingestion")
        folder_names_to_check = [
            DOCS_SOURCE_DIRECTORY,
            DOCS_PROCESSED_DIRECTORY, 
        ]
        
        self._check_folder(folder_names_to_check)

# Map file extensions to document loaders and their arguments
        self.LOADER_MAPPING = {
            ".csv": (CSVLoader, {}),
            # ".docx": (Docx2txtLoader, {}),
            ".doc": (UnstructuredWordDocumentLoader, {}),
            ".docx": (UnstructuredWordDocumentLoader, {}),
            ".enex": (EverNoteLoader, {}),
            ".eml": (MyElmLoader, {}),
            ".epub": (UnstructuredEPubLoader, {}),
            ".html": (UnstructuredHTMLLoader, {}),
            ".md": (UnstructuredMarkdownLoader, {}),
            ".odt": (UnstructuredODTLoader, {}),
            ".pdf": (PDFMinerLoader, {}),
            ".ppt": (UnstructuredPowerPointLoader, {}),
            ".pptx": (UnstructuredPowerPointLoader, {}),
            ".txt": (TextLoader, {"encoding": "utf8"}),
            # Add more mappings for other file extensions and loaders as needed
        }
 
    def load_credentials(self, credentials: dict[str, Any]) -> dict[str, Any] | None:
        pass

    def load_from_state(self):
        # get embedding model
        # get chunks of text
        texts = self._process_documents()
        # persist data
        return texts

    def _check_folder(self, folder_names):
        logger.info(f"folder_names : {folder_names}")
    # Check if the folder exists
        for folder_name in folder_names:
            if not os.path.exists(folder_name):
                logger.info(
                    f"Folder '{folder_name}' does not exist. Creating...")
                os.makedirs(folder_name)
                logger.info(f"Folder '{folder_name}' created successfully.")
            else:
                logger.info(f"Folder '{folder_name}' already exists.")

    def _load_single_document_from_filesystem(self, file_path: str) -> Document:
        logger.info(f"Loading {file_path}")
        ext = "." + file_path.rsplit(".", 1)[-1]
        if ext in self.LOADER_MAPPING:
            loader_class, loader_args = self.LOADER_MAPPING[ext]
            loader = loader_class(file_path, **loader_args)
            loaded_document = loader.load()[0]

            timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            original_file_name = os.path.basename(file_path)
            new_file_name = f"{timestamp}_{original_file_name}"
            destination_path = os.path.join(
                DOCS_PROCESSED_DIRECTORY, new_file_name)
            
            shutil.move(file_path, destination_path)
            return loaded_document

        raise ValueError(f"Unsupported file extension '{ext}'")
    
    def _load_documents_from_filesystem(self, source_dir: str, ignored_files: List[str] = []) -> List[Document]:
        """
        Loads all documents from the source documents directory, ignoring specified files
        """
        all_files = []
        for ext in self.LOADER_MAPPING:
            all_files.extend(
                glob.glob(os.path.join(
                    source_dir, f"**/*{ext}"), recursive=True)
            )
        filtered_files = [
            file_path for file_path in all_files if file_path not in ignored_files]

        logger.info(f"Found {len(filtered_files)} documents to be processed")

        with Pool(processes=os.cpu_count()) as pool:
            results = []
            with tqdm(total=len(filtered_files), desc='Loading new documents ', ncols=80) as pbar:
                for i, doc in enumerate(pool.imap_unordered(self._load_single_document_from_filesystem, filtered_files)):
                    results.append(doc)
                    pbar.update()

        return results
    
    def _process_documents(self, ignored_files: List[str] = []) -> List[Document]:
        """
        Load documents and split in chunks
        """

        logger.info(f"Loading documents from {DOCS_SOURCE_DIRECTORY}")
        documents = self._load_documents_from_filesystem(
                DOCS_SOURCE_DIRECTORY, ignored_files)

        logger.info(
            f"Loaded {len(documents)} documents from {DOCS_SOURCE_DIRECTORY}")

        if not documents:
            return None
        else:   
            return documents


if __name__ == "__main__":
    connector = LangchainFileConnector()
    credentials = {}
    connector.load_credentials(credentials)
    connector.load_from_state()
